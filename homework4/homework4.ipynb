{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:10:32.136113Z",
     "start_time": "2025-05-25T15:10:10.806308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "!pip install gensim==4.3.3\n",
    "!pip install numpy==1.18.4\n",
    "!pip install pandas==2.1.4\n",
    "\n",
    "!pip install scikit-learn\n",
    "!pip install nltk\n",
    "!pip install catboost"
   ],
   "id": "c0aa4063345e5bd2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "Requirement already satisfied: gensim==4.3.3 in /home/alexandr/miniconda3/envs/NLP_homework/lib/python3.12/site-packages (4.3.3)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /home/alexandr/miniconda3/envs/NLP_homework/lib/python3.12/site-packages (from gensim==4.3.3) (1.26.4)\r\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /home/alexandr/miniconda3/envs/NLP_homework/lib/python3.12/site-packages (from gensim==4.3.3) (1.13.1)\r\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/alexandr/miniconda3/envs/NLP_homework/lib/python3.12/site-packages (from gensim==4.3.3) (7.1.0)\r\n",
      "Requirement already satisfied: wrapt in /home/alexandr/miniconda3/envs/NLP_homework/lib/python3.12/site-packages (from smart-open>=1.8.1->gensim==4.3.3) (1.17.2)\r\n",
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "Collecting numpy==1.18.4\r\n",
      "  Using cached numpy-1.18.4.zip (5.4 MB)\r\n",
      "  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25lerror\r\n",
      "  \u001B[1;31merror\u001B[0m: \u001B[1msubprocess-exited-with-error\u001B[0m\r\n",
      "  \r\n",
      "  \u001B[31m×\u001B[0m \u001B[32mPreparing metadata \u001B[0m\u001B[1;32m(\u001B[0m\u001B[32mpyproject.toml\u001B[0m\u001B[1;32m)\u001B[0m did not run successfully.\r\n",
      "  \u001B[31m│\u001B[0m exit code: \u001B[1;36m1\u001B[0m\r\n",
      "  \u001B[31m╰─>\u001B[0m \u001B[31m[25 lines of output]\u001B[0m\r\n",
      "  \u001B[31m   \u001B[0m Running from numpy source directory.\r\n",
      "  \u001B[31m   \u001B[0m <string>:461: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates\r\n",
      "  \u001B[31m   \u001B[0m Traceback (most recent call last):\r\n",
      "  \u001B[31m   \u001B[0m   File \"/home/alexandr/miniconda3/envs/NLP_homework/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 389, in <module>\r\n",
      "  \u001B[31m   \u001B[0m     main()\r\n",
      "  \u001B[31m   \u001B[0m   File \"/home/alexandr/miniconda3/envs/NLP_homework/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 373, in main\r\n",
      "  \u001B[31m   \u001B[0m     json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\r\n",
      "  \u001B[31m   \u001B[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  \u001B[31m   \u001B[0m   File \"/home/alexandr/miniconda3/envs/NLP_homework/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 175, in prepare_metadata_for_build_wheel\r\n",
      "  \u001B[31m   \u001B[0m     return hook(metadata_directory, config_settings)\r\n",
      "  \u001B[31m   \u001B[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  \u001B[31m   \u001B[0m   File \"/tmp/pip-build-env-ce5vppcn/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 374, in prepare_metadata_for_build_wheel\r\n",
      "  \u001B[31m   \u001B[0m     self.run_setup()\r\n",
      "  \u001B[31m   \u001B[0m   File \"/tmp/pip-build-env-ce5vppcn/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 512, in run_setup\r\n",
      "  \u001B[31m   \u001B[0m     super().run_setup(setup_script=setup_script)\r\n",
      "  \u001B[31m   \u001B[0m   File \"/tmp/pip-build-env-ce5vppcn/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 317, in run_setup\r\n",
      "  \u001B[31m   \u001B[0m     exec(code, locals())\r\n",
      "  \u001B[31m   \u001B[0m   File \"<string>\", line 488, in <module>\r\n",
      "  \u001B[31m   \u001B[0m   File \"<string>\", line 465, in setup_package\r\n",
      "  \u001B[31m   \u001B[0m   File \"/tmp/pip-install-9rl_wq_7/numpy_501be2c1445d41cb89a84e9f9299d86c/numpy/distutils/__init__.py\", line 26, in <module>\r\n",
      "  \u001B[31m   \u001B[0m     from . import ccompiler\r\n",
      "  \u001B[31m   \u001B[0m   File \"/tmp/pip-install-9rl_wq_7/numpy_501be2c1445d41cb89a84e9f9299d86c/numpy/distutils/ccompiler.py\", line 111, in <module>\r\n",
      "  \u001B[31m   \u001B[0m     replace_method(CCompiler, 'find_executables', CCompiler_find_executables)\r\n",
      "  \u001B[31m   \u001B[0m                    ^^^^^^^^^\r\n",
      "  \u001B[31m   \u001B[0m NameError: name 'CCompiler' is not defined. Did you mean: 'ccompiler'?\r\n",
      "  \u001B[31m   \u001B[0m \u001B[31m[end of output]\u001B[0m\r\n",
      "  \r\n",
      "  \u001B[1;35mnote\u001B[0m: This error originates from a subprocess, and is likely not a problem with pip.\r\n",
      "\u001B[1;31merror\u001B[0m: \u001B[1mmetadata-generation-failed\u001B[0m\r\n",
      "\r\n",
      "\u001B[31m×\u001B[0m Encountered error while generating package metadata.\r\n",
      "\u001B[31m╰─>\u001B[0m See above for output.\r\n",
      "\r\n",
      "\u001B[1;35mnote\u001B[0m: This is an issue with the package mentioned above, not pip.\r\n",
      "\u001B[1;36mhint\u001B[0m: See above for details.\r\n",
      "\u001B[?25h/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "Requirement already satisfied: pandas==2.1.4 in /home/alexandr/miniconda3/envs/NLP_homework/lib/python3.12/site-packages (2.1.4)\r\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in /home/alexandr/miniconda3/envs/NLP_homework/lib/python3.12/site-packages (from pandas==2.1.4) (1.26.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/alexandr/miniconda3/envs/NLP_homework/lib/python3.12/site-packages (from pandas==2.1.4) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/alexandr/miniconda3/envs/NLP_homework/lib/python3.12/site-packages (from pandas==2.1.4) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/alexandr/miniconda3/envs/NLP_homework/lib/python3.12/site-packages (from pandas==2.1.4) (2025.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /home/alexandr/miniconda3/envs/NLP_homework/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas==2.1.4) (1.17.0)\r\n",
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "Collecting scikit-learn\r\n",
      "  Using cached scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\r\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/alexandr/miniconda3/envs/NLP_homework/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/alexandr/miniconda3/envs/NLP_homework/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\r\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\r\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\r\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\r\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\r\n",
      "Using cached scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\r\n",
      "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\r\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\r\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3/3\u001B[0m [scikit-learn][0m [scikit-learn]\r\n",
      "\u001B[1A\u001B[2KSuccessfully installed joblib-1.5.1 scikit-learn-1.6.1 threadpoolctl-3.6.0\r\n",
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "Collecting nltk\r\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\r\n",
      "Collecting click (from nltk)\r\n",
      "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Requirement already satisfied: joblib in /home/alexandr/miniconda3/envs/NLP_homework/lib/python3.12/site-packages (from nltk) (1.5.1)\r\n",
      "Collecting regex>=2021.8.3 (from nltk)\r\n",
      "  Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\r\n",
      "Collecting tqdm (from nltk)\r\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\r\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\r\n",
      "Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\r\n",
      "Using cached click-8.2.1-py3-none-any.whl (102 kB)\r\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\r\n",
      "Installing collected packages: tqdm, regex, click, nltk\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4/4\u001B[0m [nltk][32m3/4\u001B[0m [nltk]]\r\n",
      "\u001B[1A\u001B[2KSuccessfully installed click-8.2.1 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\r\n",
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "Collecting catboost\r\n",
      "  Using cached catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\r\n",
      "Collecting graphviz (from catboost)\r\n",
      "  Using cached graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\r\n",
      "Collecting matplotlib (from catboost)\r\n",
      "  Using cached matplotlib-3.10.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: numpy<3.0,>=1.16.0 in /home/alexandr/miniconda3/envs/NLP_homework/lib/python3.12/site-packages (from catboost) (1.26.4)\r\n",
      "Requirement already satisfied: pandas>=0.24 in /home/alexandr/miniconda3/envs/NLP_homework/lib/python3.12/site-packages (from catboost) (2.1.4)\r\n",
      "Requirement already satisfied: scipy in /home/alexandr/miniconda3/envs/NLP_homework/lib/python3.12/site-packages (from catboost) (1.13.1)\r\n",
      "Collecting plotly (from catboost)\r\n",
      "  Using cached plotly-6.1.1-py3-none-any.whl.metadata (6.9 kB)\r\n",
      "Requirement already satisfied: six in /home/alexandr/miniconda3/envs/NLP_homework/lib/python3.12/site-packages (from catboost) (1.17.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/alexandr/miniconda3/envs/NLP_homework/lib/python3.12/site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/alexandr/miniconda3/envs/NLP_homework/lib/python3.12/site-packages (from pandas>=0.24->catboost) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/alexandr/miniconda3/envs/NLP_homework/lib/python3.12/site-packages (from pandas>=0.24->catboost) (2025.2)\r\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->catboost)\r\n",
      "  Using cached contourpy-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\r\n",
      "Collecting cycler>=0.10 (from matplotlib->catboost)\r\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\r\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->catboost)\r\n",
      "  Using cached fonttools-4.58.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (104 kB)\r\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->catboost)\r\n",
      "  Using cached kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/alexandr/miniconda3/envs/NLP_homework/lib/python3.12/site-packages (from matplotlib->catboost) (24.2)\r\n",
      "Collecting pillow>=8 (from matplotlib->catboost)\r\n",
      "  Using cached pillow-11.2.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\r\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->catboost)\r\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\r\n",
      "Collecting narwhals>=1.15.1 (from plotly->catboost)\r\n",
      "  Using cached narwhals-1.40.0-py3-none-any.whl.metadata (11 kB)\r\n",
      "Using cached catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\r\n",
      "Using cached graphviz-0.20.3-py3-none-any.whl (47 kB)\r\n",
      "Using cached matplotlib-3.10.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\r\n",
      "Using cached contourpy-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\r\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\r\n",
      "Using cached fonttools-4.58.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\r\n",
      "Using cached kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\r\n",
      "Using cached pillow-11.2.1-cp312-cp312-manylinux_2_28_x86_64.whl (4.6 MB)\r\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\r\n",
      "Using cached plotly-6.1.1-py3-none-any.whl (16.1 MB)\r\n",
      "Using cached narwhals-1.40.0-py3-none-any.whl (357 kB)\r\n",
      "Installing collected packages: pyparsing, pillow, narwhals, kiwisolver, graphviz, fonttools, cycler, contourpy, plotly, matplotlib, catboost\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11/11\u001B[0m [catboost]/11\u001B[0m [catboost]b]\r\n",
      "\u001B[1A\u001B[2KSuccessfully installed catboost-1.2.8 contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.0 graphviz-0.20.3 kiwisolver-1.4.8 matplotlib-3.10.3 narwhals-1.40.0 pillow-11.2.1 plotly-6.1.1 pyparsing-3.2.3\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:10:42.496465Z",
     "start_time": "2025-05-25T15:10:41.958408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Загрузка датасета\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "reviews = pd.read_csv('IMDB Dataset.csv')\n",
    "reviews"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T14:12:10.201591Z",
     "start_time": "2025-05-25T14:12:10.188467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Распределение целевой переменной\n",
    "\n",
    "reviews['sentiment'].value_counts(dropna=False)"
   ],
   "id": "1459be3d8e55afb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    25000\n",
       "negative    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:10:50.528150Z",
     "start_time": "2025-05-25T15:10:50.260159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Разбиваем выборку на две части; тренировочную (train) и тестовую выборку (test)\n",
    "\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "X_train,X_test = tts(reviews,test_size=0.2,random_state=32)"
   ],
   "id": "8a1f6788dcc24831",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:10:53.478127Z",
     "start_time": "2025-05-25T15:10:53.464936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Тренировачная выборка\n",
    "\n",
    "corpus_tr = list(X_train['review'])  # корпус\n",
    "target_tr = list(X_train['sentiment'])   # целевая функция"
   ],
   "id": "52262abc8139130c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:10:58.820224Z",
     "start_time": "2025-05-25T15:10:58.815526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Тестовая выборка\n",
    "\n",
    "corpus_te = list(X_test['review'])  # корпус\n",
    "target_te = list(X_test['sentiment'])   # целевая функция"
   ],
   "id": "29a4356bfcc6b070",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:11:26.785069Z",
     "start_time": "2025-05-25T15:11:07.204333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Все варианты преобразования документов в корпусе используя TF-IDF для создания векторного представления слов перечислены ниже\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "# минимальная частота слов\n",
    "v1_vectoriser = TfidfVectorizer(min_df=1,norm=None)\n",
    "v1_vectoriser.fit(corpus_tr)\n",
    "X1 = v1_vectoriser.transform(corpus_tr)\n",
    "\n",
    "v2_vectoriser = TfidfVectorizer(min_df=2,norm=None)\n",
    "v2_vectoriser.fit(corpus_tr)\n",
    "X2 = v2_vectoriser.transform(corpus_tr)\n",
    "\n",
    "v3_vectoriser = TfidfVectorizer(min_df=3,norm=None)\n",
    "v3_vectoriser.fit(corpus_tr)\n",
    "X3 = v3_vectoriser.transform(corpus_tr)"
   ],
   "id": "547d47605e12d70d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/alexandr/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:11:52.042260Z",
     "start_time": "2025-05-25T15:11:26.800403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# стоп слова варианты\n",
    "v4_vectoriser = TfidfVectorizer(min_df=1,norm=None,stop_words=russian_stopwords)\n",
    "v4_vectoriser.fit(corpus_tr)\n",
    "X4 = v4_vectoriser.transform(corpus_tr)\n",
    "\n",
    "v5_vectoriser = TfidfVectorizer(min_df=2,norm=None,stop_words=russian_stopwords)\n",
    "v5_vectoriser.fit(corpus_tr)\n",
    "X5 = v5_vectoriser.transform(corpus_tr)\n",
    "\n",
    "v6_vectoriser = TfidfVectorizer(min_df=3,norm=None,stop_words=russian_stopwords)\n",
    "v6_vectoriser.fit(corpus_tr)\n",
    "X6 = v6_vectoriser.transform(corpus_tr)"
   ],
   "id": "101ea97e850358e8",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T10:41:07.189964Z",
     "start_time": "2025-05-25T10:38:56.527913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# н-грам варианты\n",
    "v7_vectoriser = TfidfVectorizer(min_df=3,norm=None,stop_words=russian_stopwords,ngram_range=(1,1))\n",
    "v7_vectoriser.fit(corpus_tr)\n",
    "X7 = v7_vectoriser.transform(corpus_tr)\n",
    "\n",
    "v8_vectoriser = TfidfVectorizer(min_df=3,norm=None,stop_words=russian_stopwords,ngram_range=(1,2))\n",
    "v8_vectoriser.fit(corpus_tr)\n",
    "X8 = v8_vectoriser.transform(corpus_tr)\n",
    "\n",
    "v9_vectoriser = TfidfVectorizer(min_df=3,norm=None,stop_words=russian_stopwords,ngram_range=(1,3))\n",
    "v9_vectoriser.fit(corpus_tr)\n",
    "X9 = v9_vectoriser.transform(corpus_tr)\n",
    "\n",
    "v10_vectoriser = TfidfVectorizer(min_df=3,norm=None,stop_words=russian_stopwords,ngram_range=(2,2))\n",
    "v10_vectoriser.fit(corpus_tr)\n",
    "X10 = v10_vectoriser.transform(corpus_tr)\n",
    "\n",
    "v11_vectoriser = TfidfVectorizer(min_df=3,norm=None,stop_words=russian_stopwords,ngram_range=(3,3))\n",
    "v11_vectoriser.fit(corpus_tr)\n",
    "X11 = v11_vectoriser.transform(corpus_tr)\n",
    "\n",
    "v12_vectoriser = TfidfVectorizer(min_df=3,norm=None,stop_words=russian_stopwords,ngram_range=(4,4))\n",
    "v12_vectoriser.fit(corpus_tr)\n",
    "X12 = v12_vectoriser.transform(corpus_tr)"
   ],
   "id": "5daf69ad839d7670",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T10:41:32.083635Z",
     "start_time": "2025-05-25T10:41:07.212315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# максимальная частота слов\n",
    "v13_vectoriser = TfidfVectorizer(min_df=3,max_df=0.7,norm=None,stop_words=russian_stopwords)\n",
    "v13_vectoriser.fit(corpus_tr)\n",
    "X13 = v13_vectoriser.transform(corpus_tr)\n",
    "\n",
    "v14_vectoriser = TfidfVectorizer(min_df=3,max_df=0.8,norm=None,stop_words=russian_stopwords)\n",
    "v14_vectoriser.fit(corpus_tr)\n",
    "X14 = v14_vectoriser.transform(corpus_tr)\n",
    "\n",
    "v15_vectoriser = TfidfVectorizer(min_df=3,max_df=0.9,norm=None,stop_words=russian_stopwords)\n",
    "v15_vectoriser.fit(corpus_tr)\n",
    "X15 = v15_vectoriser.transform(corpus_tr)"
   ],
   "id": "c5e3e4f1996ead66",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:11:52.101215Z",
     "start_time": "2025-05-25T15:11:52.057261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# В плане моделей, попробуем два подхода:\n",
    "# - случайный лес (который использует бэггинг и метод случайного пространства)\n",
    "# - градиентный бустинг СatBoost\n",
    "# Начнем с базового RandomForest (не очень глубокого) model_srf, а потом попробуем улучшить результат с помощью model_drf и model_ocb\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "model_srf = RandomForestClassifier(n_estimators=10,random_state=32)  # не глубокий случайный лес\n",
    "model_drf = RandomForestClassifier(n_estimators=40,random_state=32)  # глубокий случайный лес\n",
    "model_ocb = CatBoostClassifier(silent=True)  # градиентный бустинг"
   ],
   "id": "1bcdd60e914101fd",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T14:19:12.489121Z",
     "start_time": "2025-05-25T14:19:12.480968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# вспомогательная функция для оценки модели\n",
    "def evaluate_tfidf(X,vect,name,model):\n",
    "\n",
    "    print(f'case_id: :{name}')\n",
    "    print('==================================')\n",
    "\n",
    "    # train model\n",
    "    model.fit(X,target_tr)\n",
    "    y_model = model.predict(X)\n",
    "    print(f'train: {accuracy_score(target_tr,y_model)}')\n",
    "\n",
    "    X = vect.transform(corpus_te)\n",
    "    y_model = model.predict(X)\n",
    "    print(f'test: {accuracy_score(y_model,target_te)}')\n",
    "\n",
    "    print('==================================','\\n')"
   ],
   "id": "6483098baf51de77",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T10:35:34.652906Z",
     "start_time": "2025-05-25T10:34:54.507040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Проверим как влияет фильтр минимальной частоты слов (min_df) в документе на результат#\n",
    "# v1 (min_df=1)\n",
    "# v2 (min_df=2 фильтруем слова, которые появляются меньше 2 раз)\n",
    "# v3 (min_df=3 фильтруем слова, которые появляются меньше 3 раз)\n",
    "# Как мы видим, эффект от фильтрации часто встречающихся слов разный\n",
    "\n",
    "evaluate_tfidf(X1,v1_vectoriser,'v1',model_srf)\n",
    "evaluate_tfidf(X2,v2_vectoriser,'v2',model_srf)\n",
    "evaluate_tfidf(X3,v3_vectoriser,'v3',model_srf)"
   ],
   "id": "b8de1643af36c9a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case_id: :v1\n",
      "==================================\n",
      "train: 0.994575\n",
      "test: 0.7549\n",
      "================================== \n",
      "\n",
      "case_id: :v2\n",
      "==================================\n",
      "train: 0.993675\n",
      "test: 0.7555\n",
      "================================== \n",
      "\n",
      "case_id: :v3\n",
      "==================================\n",
      "train: 0.99375\n",
      "test: 0.7584\n",
      "================================== \n",
      "\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T14:21:57.418250Z",
     "start_time": "2025-05-25T14:20:50.689746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Попробуем повысить точность с помощью фильтрации не релевантных слов (стоп слова)\n",
    "# При использовании разных настроек min_df точность модели меняется по разному; min_df=3 дает наиболее хороший результат\n",
    "# Вариатны:\n",
    "# v4 (фильтрация стоп слов с min_df=1)\n",
    "# v5 (фильтрация стоп слов с min_df=2)\n",
    "# v6 (фильтрация стоп слов с min_df=3)\n",
    "\n",
    "evaluate_tfidf(X4,v4_vectoriser,'v4',model_srf)\n",
    "evaluate_tfidf(X5,v5_vectoriser,'v5',model_srf)\n",
    "evaluate_tfidf(X6,v6_vectoriser,'v6',model_srf)"
   ],
   "id": "98fdddd43ee1cd52",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case_id: :v4\n",
      "==================================\n",
      "train: 0.994575\n",
      "test: 0.7549\n",
      "================================== \n",
      "\n",
      "case_id: :v5\n",
      "==================================\n",
      "train: 0.993675\n",
      "test: 0.7555\n",
      "================================== \n",
      "\n",
      "case_id: :v6\n",
      "==================================\n",
      "train: 0.99375\n",
      "test: 0.7584\n",
      "================================== \n",
      "\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T10:42:59.380066Z",
     "start_time": "2025-05-25T10:41:32.105136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Вариант v6 дает нам точность 0.7584 на тестовой выборке, используем этот вариант и проверим разные комбинации н-грамм\n",
    "# Как мы видим, есть улучшения на тестовой выборке при использовании биграмм и триграмм\n",
    "# Варианты:\n",
    "# v7 ngram_range=(1,1) (Только униграммы)\n",
    "# v8 ngram_range=(1,2) (Униграммы и биграммы)\n",
    "# v9 ngram_range=(1,3) (униграммы, биграммы и триграммы)\n",
    "\n",
    "evaluate_tfidf(X7,v7_vectoriser,'v7',model_srf)\n",
    "evaluate_tfidf(X8,v8_vectoriser,'v8',model_srf)\n",
    "evaluate_tfidf(X9,v9_vectoriser,'v9',model_srf)"
   ],
   "id": "d4be7ddb88e5667",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case_id: :v7\n",
      "==================================\n",
      "train: 0.99375\n",
      "test: 0.7584\n",
      "================================== \n",
      "\n",
      "case_id: :v8\n",
      "==================================\n",
      "train: 0.9946\n",
      "test: 0.7633\n",
      "================================== \n",
      "\n",
      "case_id: :v9\n",
      "==================================\n",
      "train: 0.99435\n",
      "test: 0.7778\n",
      "================================== \n",
      "\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T10:54:45.747336Z",
     "start_time": "2025-05-25T10:52:45.066811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Проверим еще варианты:\n",
    "# v10 ngram_range=(2,2) (Только биграммы)\n",
    "# v11 ngram_range=(3,3) (Только триграммы)\n",
    "# v12 ngram_range=(4,4) (Только 4-граммы)\n",
    "\n",
    "evaluate_tfidf(X10,v10_vectoriser,'v10',model_srf)\n",
    "evaluate_tfidf(X11,v11_vectoriser,'v11',model_srf)\n",
    "evaluate_tfidf(X12,v12_vectoriser,'v12',model_srf)"
   ],
   "id": "91cf486c60117b95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case_id: :v10\n",
      "==================================\n",
      "train: 0.993325\n",
      "test: 0.7597\n",
      "================================== \n",
      "\n",
      "case_id: :v11\n",
      "==================================\n",
      "train: 0.9907\n",
      "test: 0.7571\n",
      "================================== \n",
      "\n",
      "case_id: :v12\n",
      "==================================\n",
      "train: 0.98855\n",
      "test: 0.7266\n",
      "================================== \n",
      "\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T10:57:55.182450Z",
     "start_time": "2025-05-25T10:57:13.100335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# В задании так же упоминается максимальная частота слов max_df в векторизаторе, попробуем варианты:#\n",
    "# v13 max_df=0.7 (фильтруем слова, которые появляются в 70% документов)\n",
    "# v14 max_df=0.8 (фильтруем слова, которые появляются в 80% документов)\n",
    "# v15 max_df=0.9 (фильтруем слова, которые появляются в 90% документов)\n",
    "# Как мы видим, лучший результат при max_df=0.8\n",
    "\n",
    "evaluate_tfidf(X13,v13_vectoriser,'v13',model_srf)\n",
    "evaluate_tfidf(X14,v14_vectoriser,'v14',model_srf)\n",
    "evaluate_tfidf(X15,v15_vectoriser,'v15',model_srf)"
   ],
   "id": "f2dd55d72ad87d45",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case_id: :v13\n",
      "==================================\n",
      "train: 0.994075\n",
      "test: 0.7589\n",
      "================================== \n",
      "\n",
      "case_id: :v14\n",
      "==================================\n",
      "train: 0.9943\n",
      "test: 0.7594\n",
      "================================== \n",
      "\n",
      "case_id: :v15\n",
      "==================================\n",
      "train: 0.993825\n",
      "test: 0.759\n",
      "================================== \n",
      "\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:32:51.953272Z",
     "start_time": "2025-05-25T15:32:51.948821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# У случайного леса много параметров, которые можно перепробовать, ограничимся:\n",
    "# max_depth максимальная глубина разбиения\n",
    "# n_estimators количество решающих деревьев\n",
    "# criterion критерии оценки разбиения в узле\n",
    "# min_samples_leaf минимальное количество образцов, необходимое для нахождения в листовом узле каждого дерева\n",
    "# min_samples_split Минимальное количество образцов, необходимое для разбиения внутреннего узла каждого дерева\n",
    "# Из соображения времени, количество построенных деревьев ограничивается (n_estimators=100)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "def evaluate_cv(X,name,base_estimator):\n",
    "\n",
    "    print(f'case_id: :{name}')\n",
    "    print('==================================')\n",
    "\n",
    "    rs_space={'max_depth':list(np.arange(10,60,10)) + [None],\n",
    "              'n_estimators':np.arange(40,120, step=20),\n",
    "              'criterion':['gini','entropy'],\n",
    "              'min_samples_leaf':[1,2,3,4],\n",
    "              'min_samples_split':np.arange(2,6, step=2),\n",
    "              }\n",
    "\n",
    "    model = GridSearchCV(base_estimator,\n",
    "                         rs_space,\n",
    "                         scoring='accuracy',\n",
    "                         n_jobs=-1,\n",
    "                         cv=3)\n",
    "\n",
    "    # train model\n",
    "    model.fit(X,target_tr)\n",
    "\n",
    "    print('Best hyperparameters are: '+str(model.best_params_))\n",
    "    print('Best score is: '+str(model.best_score_))\n",
    "\n",
    "    print('==================================','\\n')\n"
   ],
   "id": "fb1031a680be535e",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T12:12:39.741686Z",
     "start_time": "2025-05-25T10:59:44.731798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Кросс валидация нам дает немного хуже результат чем на train/test разбиении\n",
    "# в итоге мы получаем точность 0.857 на тестовой выборке в cv, что не так уж и плохо\n",
    "\n",
    "model_drf = RandomForestClassifier(random_state=32)\n",
    "evaluate_cv(X6,'random_forest_cv',model_drf)"
   ],
   "id": "fc537a0e367d4648",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case_id: :random_forest_cv\n",
      "==================================\n",
      "Best hyperparameters are: {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': np.int64(2), 'n_estimators': np.int64(100)}\n",
      "Best score is: 0.8569999929464264\n",
      "================================== \n",
      "\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T12:35:30.598421Z",
     "start_time": "2025-05-25T12:34:17.296789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Проверим точность на всей тестовой выборке\n",
    "\n",
    "best_model = RandomForestClassifier(**{'criterion': 'entropy',\n",
    "                                     'max_depth': None,\n",
    "                                     'min_samples_leaf': 4,\n",
    "                                     'min_samples_split': 2,\n",
    "                                     'n_estimators': 100,\n",
    "                                     'random_state': 32})\n",
    "\n",
    "evaluate_tfidf(X6,v6_vectoriser,'v6_rf_best',best_model)"
   ],
   "id": "305ab5d6ef964fcb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case_id: :v6_rf_best\n",
      "==================================\n",
      "train: 0.956575\n",
      "test: 0.8539\n",
      "================================== \n",
      "\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:11:58.270019Z",
     "start_time": "2025-05-25T15:11:58.259703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Выгрузим TF-IDF веса из v6_vectoriser\n",
    "# Создадим словарь\n",
    "tfidf_weights = dict(zip(list(v6_vectoriser.vocabulary_.keys()),v6_vectoriser.idf_))"
   ],
   "id": "b082f6af9158039a",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:12:01.591884Z",
     "start_time": "2025-05-25T15:12:01.521693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Для нашей задачи возьмем предобученные эмбеддинги (варианты без тагсет)\n",
    "# geowac_lemmas_none_fasttextskipgram_300_5_2020\n",
    "\n",
    "import gensim\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# название и URL\n",
    "we_models = {\"geowac_lemmas_none_fasttextskipgram_300_5_2020\": \"http://vectors.nlpl.eu/repository/20/213.zip\",}"
   ],
   "id": "68cff7c44f671229",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:15:37.490020Z",
     "start_time": "2025-05-25T12:53:12.342764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# сохраняем модель\n",
    "def get_models(model_url, model_name):\n",
    "    model_path = model_name + \".zip\"\n",
    "    urllib.request.urlretrieve(model_url, model_path)\n",
    "\n",
    "for model_name, model_url in we_models.items():\n",
    "    get_models(model_url, model_name)"
   ],
   "id": "bc5df9afd7a2002c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:12:20.316492Z",
     "start_time": "2025-05-25T15:12:20.311919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Функция для чтения word2vec / FastText\n",
    "\n",
    "def open_model(model_name, is_fasttext = True):\n",
    "\n",
    "    # word2vec (model.bin)\n",
    "    if is_fasttext == False:\n",
    "        model_file = model_name + \".zip\"\n",
    "        with zipfile.ZipFile(model_file, 'r') as archive:\n",
    "            stream = archive.open('model.bin')\n",
    "            model = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)\n",
    "\n",
    "    # fasttext (model.model)\n",
    "    else:\n",
    "        model_file = model_name\n",
    "        model = gensim.models.KeyedVectors.load(model_file + \"/model.model\")\n",
    "    return model"
   ],
   "id": "c9984a0e1ef4fbb",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:16:55.550197Z",
     "start_time": "2025-05-25T13:16:40.887472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Распакуем эмбеддинги и загружаем вектора\n",
    "\n",
    "with zipfile.ZipFile(\"geowac_lemmas_none_fasttextskipgram_300_5_2020.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"./working/geowac_lemmas_none_fasttextskipgram_300_5_2020\")"
   ],
   "id": "74c27277fcfd1a89",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:12:38.481256Z",
     "start_time": "2025-05-25T15:12:35.524560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# загружаем KeyedVectors эмбеддинговый вектора\n",
    "geowac_model = open_model('./working/geowac_lemmas_none_fasttextskipgram_300_5_2020')"
   ],
   "id": "d234e7a607808251",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:19:52.190113Z",
     "start_time": "2025-05-25T15:19:45.743471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Токенизируем все документы в тренировочной выборке/копрусе, будем использовать стандартный метод из nltk word_tokenize\n",
    "# Токенизированные документы сохраним в lst_corpus_tr и lst_corpus_te для train и test выборки\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Preprocessing, returns list instead\n",
    "def tokenize_for_word2vec(text):\n",
    "\n",
    "    text = text.lower() #changes to lower case\n",
    "    tokens = word_tokenize(text) #tokenize the text\n",
    "\n",
    "    clean_list = []\n",
    "    for token in tokens:\n",
    "        clean_list.append(token)\n",
    "\n",
    "    return clean_list"
   ],
   "id": "f9643771c59012e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/alexandr/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:20:27.952228Z",
     "start_time": "2025-05-25T15:19:58.975212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lst_corpus_tr = []\n",
    "for doc in corpus_tr:\n",
    "    lst_corpus_tr.append(tokenize_for_word2vec(doc))\n",
    "\n",
    "lst_corpus_te = []\n",
    "for doc in corpus_te:\n",
    "    lst_corpus_te.append(tokenize_for_word2vec(doc))"
   ],
   "id": "8329ea7ecdaa794b",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:21:40.762637Z",
     "start_time": "2025-05-25T15:21:40.758776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Воспользуемся самым простым подходом; для каждого документа мы возьмем усредненный эмбеддинговый вектор всех слов которые в него входят\n",
    "# Сохраняем эмбеддинг для каждого документа в X_tr и X_te для train и test выборки\n",
    "\n",
    "# Get average embedding vector for each text\n",
    "import numpy as np\n",
    "\n",
    "def doc_vectorizer(doc, model):\n",
    "\n",
    "    doc_vector = []\n",
    "    num_words = 0\n",
    "\n",
    "    for word in doc:\n",
    "        try:\n",
    "            if num_words == 0:\n",
    "                doc_vector = model[word]\n",
    "            else:\n",
    "                doc_vector = np.add(doc_vector, model[word])\n",
    "            num_words += 1\n",
    "        except:\n",
    "            pass  # if embedding vector isn't found\n",
    "\n",
    "    return np.asarray(doc_vector) / num_words"
   ],
   "id": "848dfb7ba7845992",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:22:37.494624Z",
     "start_time": "2025-05-25T15:21:42.518228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_tr = []\n",
    "for doc in lst_corpus_tr:\n",
    "    X_tr.append(doc_vectorizer(doc,geowac_model))\n",
    "\n",
    "\n",
    "X_te = []\n",
    "for doc in lst_corpus_te:\n",
    "    X_te.append(doc_vectorizer(doc,geowac_model))"
   ],
   "id": "603d9d74bf50ce87",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:22:37.530020Z",
     "start_time": "2025-05-25T15:22:37.527079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Мы сохранили веса значимости слов (токенов) в документах корпуса в tfidf_weights\n",
    "# Сохраняем взвешенные эмбеддинги для каждого документа в Xw_tr и Xw_te для train и test выборки\n",
    "\n",
    "# Get average embedding vector for each text\n",
    "\n",
    "def doc_vectorizer_tfidf(doc, model):\n",
    "\n",
    "    doc_vector = []\n",
    "    num_words = 0\n",
    "\n",
    "    for word in doc:\n",
    "        try:\n",
    "            if num_words == 0:\n",
    "\n",
    "                # if word is in tfidf dictionary\n",
    "                if(word in tfidf_weights):\n",
    "                    doc_vector = model[word]*(tfidf_weights[word])\n",
    "                else:\n",
    "                    doc_vector = model[word]\n",
    "            else:\n",
    "\n",
    "                # if word is in tfidf dictionary\n",
    "                if(word in tfidf_weights):\n",
    "                    doc_vector = np.add(doc_vector, model[word]*(tfidf_weights[word]))\n",
    "                else:\n",
    "                    doc_vector = np.add(doc_vector, model[word])\n",
    "\n",
    "            num_words += 1\n",
    "        except:\n",
    "            pass  # if embedding vector isn't found\n",
    "\n",
    "    return np.asarray(doc_vector) / num_words"
   ],
   "id": "d9d7f48071325fe8",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:23:54.765583Z",
     "start_time": "2025-05-25T15:22:37.600715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Xw_tr = []\n",
    "for doc in lst_corpus_tr:\n",
    "    Xw_tr.append(doc_vectorizer_tfidf(doc,geowac_model))\n",
    "\n",
    "Xw_te = []\n",
    "for doc in lst_corpus_te:\n",
    "    Xw_te.append(doc_vectorizer_tfidf(doc,geowac_model))"
   ],
   "id": "5ce479c46f43eaf9",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:23:54.801554Z",
     "start_time": "2025-05-25T15:23:54.798760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# В плане моделей, попробуем два подхода случайный лес и градиентный бустинг СatBoost\n",
    "# Начнем с базового RandomForest (не очень глубокого) model_srf, а потом попробуем улучшить результат с помощью model_drf и model_ocb\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "model_srf = RandomForestClassifier(n_estimators=10,random_state=32)\n",
    "model_drf = RandomForestClassifier(n_estimators=40,random_state=32)\n",
    "model_ocb = CatBoostClassifier(silent=True)"
   ],
   "id": "91271d5adda9420f",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:23:54.932412Z",
     "start_time": "2025-05-25T15:23:54.929436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Импортируем эмбеддинги X_tr, X_te\n",
    "# target_tr, target_te\n",
    "\n",
    "def evaluate_embedding(X_tr,X_te,name,model):\n",
    "\n",
    "    print(f'case_id: :{name}')\n",
    "    print('==================================')\n",
    "\n",
    "    # train model\n",
    "    model.fit(X_tr,target_tr)\n",
    "    y_model = model.predict(X_tr)\n",
    "    print(f'train: {accuracy_score(target_tr,y_model)}')\n",
    "\n",
    "    y_model = model.predict(X_te)\n",
    "    print(f'test: {accuracy_score(y_model,target_te)}')\n",
    "\n",
    "    print('==================================','\\n')"
   ],
   "id": "81921ee7cbf04a06",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:28:45.587607Z",
     "start_time": "2025-05-25T15:26:33.137371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Модели без TF-IDF весами для эмбеддингов, как и раньше попробуем базовые модели, после чего мы попытаемся оптимизировать гиперпараметры\n",
    "\n",
    "evaluate_embedding(X_tr,X_te,'geowac_rf',model_drf)"
   ],
   "id": "ce6fd92a7beb6a5c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case_id: :geowac_rf\n",
      "==================================\n",
      "train: 1.0\n",
      "test: 0.7062\n",
      "================================== \n",
      "\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:29:31.342035Z",
     "start_time": "2025-05-25T15:28:58.010191Z"
    }
   },
   "cell_type": "code",
   "source": "evaluate_embedding(X_tr,X_te,'geowac_cat',model_ocb)",
   "id": "fc7ebbf617618a04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case_id: :geowac_cat\n",
      "==================================\n",
      "train: 0.873275\n",
      "test: 0.7653\n",
      "================================== \n",
      "\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:31:02.674311Z",
     "start_time": "2025-05-25T15:29:31.402209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Модели с TF-IDF весами для эмбеддингов\n",
    "# Точность с весами не улучшилась\n",
    "\n",
    "evaluate_embedding(Xw_tr,Xw_te,'geowac_rf_tfidf_weights',model_drf)"
   ],
   "id": "fc81e3324c3a7f93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case_id: :geowac_rf_tfidf_weights\n",
      "==================================\n",
      "train: 1.0\n",
      "test: 0.7003\n",
      "================================== \n",
      "\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:32:17.495352Z",
     "start_time": "2025-05-25T15:31:02.727047Z"
    }
   },
   "cell_type": "code",
   "source": "evaluate_embedding(Xw_tr,Xw_te,'geowac_cat_tfidf_weights',model_ocb)",
   "id": "7fac48e95afc5e3a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case_id: :geowac_cat_tfidf_weights\n",
      "==================================\n",
      "train: 0.87305\n",
      "test: 0.763\n",
      "================================== \n",
      "\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T18:16:49.313473Z",
     "start_time": "2025-05-25T15:38:00.666478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Воспользуемся той же функции, что и раньше для того, чтобы оптимизировать гиперпараметры модели\n",
    "# Будем использовать случайный лес и найдем наилучшие гиперпараметры используя кросс-валидацию GridSearchCV\n",
    "\n",
    "model_drf = RandomForestClassifier(random_state=32)\n",
    "evaluate_cv(np.array(X_tr),'geowac_rf_cv',model_drf)"
   ],
   "id": "f6ba9507b6e08c06",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case_id: :geowac_rf_cv\n",
      "==================================\n",
      "Best hyperparameters are: {'criterion': 'entropy', 'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best score is: 0.7084500777586186\n",
      "================================== \n",
      "\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T18:17:39.519449Z",
     "start_time": "2025-05-25T18:17:39.514552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_model_rf = RandomForestClassifier(**{'criterion': 'entropy',\n",
    "                                       'max_depth': 20,\n",
    "                                       'min_samples_leaf': 4,\n",
    "                                       'min_samples_split': 2,\n",
    "                                       'n_estimators': 100,\n",
    "                                       'random_state':32})"
   ],
   "id": "9bc0dcb3d7ab4181",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T18:19:05.616675Z",
     "start_time": "2025-05-25T18:17:41.628193Z"
    }
   },
   "cell_type": "code",
   "source": "evaluate_embedding(X_tr,X_te,'geowac_rf_tfidf_weights',best_model_rf)",
   "id": "d16105d56a955934",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case_id: :geowac_rf_tfidf_weights\n",
      "==================================\n",
      "train: 0.9992\n",
      "test: 0.709\n",
      "================================== \n",
      "\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T20:56:05.061536Z",
     "start_time": "2025-05-25T18:19:05.651072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Повторяем оптимизацию гиперпараметров для эмбеддингов с весами TF-IDF\n",
    "# Умножение весов из TF-IDF на эмбеддинги не намного улучшило модель\n",
    "\n",
    "model_drf = RandomForestClassifier(random_state=32)\n",
    "evaluate_cv(np.array(Xw_tr),'geowac_rf_cv_tfidf_weights',model_drf)"
   ],
   "id": "20f131c0cb2dc562",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case_id: :geowac_rf_cv_tfidf_weights\n",
      "==================================\n",
      "Best hyperparameters are: {'criterion': 'entropy', 'max_depth': 40, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best score is: 0.709975001512431\n",
      "================================== \n",
      "\n"
     ]
    }
   ],
   "execution_count": 44
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
